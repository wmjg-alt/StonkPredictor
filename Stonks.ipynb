{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL REPORT \n",
    "    Taking a look at Yfinance, stock information\n",
    "    Looking at data from 2 angles: Regression and Classification\n",
    "    \n",
    "    We take a stock, fetch its history from Yahoo Finanace.\n",
    "    Then we build a feature set of the previous x days, and the target y is either:\n",
    "        A) The Closing price of the subsequent day.\n",
    "        B) Whether that closing price is higher or lower than 'today's closing price.\n",
    "    Some stocks have a history of tending to go up, and the result for classification is 'too good'.\n",
    "    The class TO_THE_MOON looks into this, acting as a model that assumes a stock will always go up.\n",
    "    I wanted to select a well-known target stock that wasn't biased too heavily to go up/down.\n",
    "    \n",
    "#### REGRESSION\n",
    "    So, we start with a data set of Microsoft daily stock information.\n",
    "    We make an X out of the previous 30 days and y = Closing value of 'tomorrow'\n",
    "    And if we throw this into a bunch of BASIC models with little monte carlo splits, we get...\n",
    "        0.9994653230315521\t0.9993490407573127\n",
    "        0.9987841453242439\t0.9988047837616991\n",
    "        0.999476371806644\t0.9991202017873585\n",
    "        0.36916190336346727\t0.3546131522438993\n",
    "        0.9994602018307008\t0.9993484792927727\n",
    "        0.9991561648785652\t0.9991232673696759\n",
    "        0.9999045527516056\t0.9993110397786895\n",
    "    ... Well that's odd.\n",
    "    This is what spawned the desire to look at classification. \n",
    "    These predictions are being scored by how close they are to the target. \n",
    "    And given stock data, MOST of the time, the changes aren't hugely volatile (this stock and others I checked).\n",
    "    here's what a random sample looks like:\n",
    "        [26.120001 26.129999 26.139999 26.160000 25.990000 25.709999 26.049999\n",
    "         25.969999 25.750000 26.030001 25.700001 25.469999 26.000000 25.820000\n",
    "         25.870001 26.090000 25.629999 26.030001 26.160000 26.320000 26.350000\n",
    "         26.190001 26.590000 26.510000 27.010000 27.160000 27.450001 27.400000\n",
    "         25.510000 25.360001 26.129999]] \n",
    "         y = [25.889999]\n",
    "* ALL OF THE VALUES ARE WITHIN A //TINY\\\\ margin of error\n",
    "* So we're not predicting whether \"tomorrow\" is better than \"today\", just a close number\n",
    "* This spawned the idea to tackle it with CLASSIFICATION\n",
    "\n",
    "#### CLASSIFICATION   \n",
    "    So the data is now, X = the delta between today and each of the 30 days before.\n",
    "    and y = Is the delta between today and tomorrow > 0 <- BOOLEAN, so we're 0 or 1 classification\n",
    "    Right off the bat, let's run a series of classification models with little MC splits\n",
    "        DIAMONDHANDS\t0.5138681169272603\t0.5118486633439058\n",
    "        NaiveBayes\t    0.5300475866757307\t0.5338468509288627\n",
    "        SVClassifier\t0.5901880806707457\t0.5811961939284096\n",
    "        KNeighbors\t    0.7208021753908905\t0.5583824195740824\n",
    "        DecisionTree\t1.0\t                0.6083597643860444\n",
    "        RandomForest\t1.0\t                0.6867240598096964\n",
    "        SGDClass\t    0.5909585316111489\t0.5804259175351156\n",
    "    So DIAMONDHANDS is the TO_THE_MOON model. So we can see, there's a good variety int he data. \n",
    "    Today I learned that 51% of the time Microsoft goes up in value.\n",
    "    And we already have improved performance with RandomForest.\n",
    "    Here's what a random data point looks like in the classifiers:\n",
    "        [[0.000000 0.019999 -0.100000 -0.060001 -0.010000 0.029999 0.369999\n",
    "          0.240000 0.400000 0.509998 0.299999 0.589998 0.269999 0.029999 0.189999\n",
    "          0.189999 0.199999 0.349998 0.009998 -0.170000 -0.200001 -0.220001\n",
    "          0.000000 -0.350000 -0.430000 -1.090000 -1.110001 -1.410002 0.179998\n",
    "          0.679998 26.129999]] \n",
    "          y = [0.000000]\n",
    "* So, let's use the GridSearch ideas from PS8 on that random forest.\n",
    "            \n",
    "        max_depth = [5,10,15,20,25] \n",
    "        max_features = [3,5,10]\n",
    "        n_estimators = [10,25,100]\n",
    "        \n",
    "                                WINNER:\n",
    "             rank   test score   depth  features   estimators\n",
    "              1      0.705029      5       20         100\n",
    "        And then cross_val = 10\n",
    "            train: 0.72215286742883\n",
    "            test: 0.6873037701301736\n",
    "* I'd call that mildly successful. The small RandomForest monte carlo averaged 0.6867. BUT we managed 0.6873 in the cross val. Which means that's a consistent predictive rate, not just an average. And on individual splits that might be underfit/overfit/biased etc, we managed to average over 0.7 - Which is an improvement from the naive model that got 0.51.\n",
    "\n",
    "#### Future Endeavors\n",
    "* I was thinking we need more features. Or rather, better features. Examples:\n",
    "        Data on an adjacent ETF could be used as predictive.\n",
    "            day growth, week growth, month growth in relevant ETF\n",
    "            \n",
    "        day   High and low, and growth\n",
    "        month high and low, and growth\n",
    "        year  high and low, and growth\n",
    "* The trouble is that it would limit the data set. as dates within a year of the opening of the stock would go uncounted\n",
    "* I tried to get some fun neural net stuff up and working in time, but it became overwhelmingly fiddly.\n",
    "        -An adventure for another day-       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to look at stocks, with Yfinance and more\n",
    "import yfinance as yf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT MODELS\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#import tensorflow as tf\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(stock, pd, inty):\n",
    "    i = yf.download(tickers=stock,period=pd,interval=inty)\n",
    "    return i.to_numpy()\n",
    "\n",
    "Yfin_map = {\"Open\":0, \"High\":1, \"Low\":2,\"Close\":3, \"Adj Close\":4, \"Volume\":5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_increases(df):\n",
    "    y2 = np.zeros((len(df),))\n",
    "    for i,row in enumerate(df):\n",
    "        if i != len(df)-1:\n",
    "            tmp = df[i+1][Yfin_map['Close']] - row[Yfin_map['Open']]\n",
    "            y2[i] = tmp > 0\n",
    "    return y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_increases(history,jump=5):\n",
    "    X_pile= np.zeros((len(history), jump+1))\n",
    "    blank = [0 for x in range(0,jump+1)]\n",
    "    for i,row in enumerate(history):\n",
    "        if i<jump or i == len(history)-1:\n",
    "            pass\n",
    "        else:\n",
    "            X_pile[i][0:jump] = np.array([row[Yfin_map['Close']] - history[i-j][Yfin_map['Close']] for j in range(0,jump)])\n",
    "            X_pile[i][jump] = row[Yfin_map['Close']]\n",
    "    return X_pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's Make the Y, \"TOMORROW\"s CLOSE\n",
    "def make_Y(df):\n",
    "    y = np.zeros((len(df),))\n",
    "    for i,row in enumerate(df):\n",
    "        if i != len(df)-1:\n",
    "            y[i] = df[i+1][Yfin_map['Close']]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on day's opening values\n",
    "def make_X(history, jump=5):\n",
    "    X_pile= np.zeros((len(history), jump+1))\n",
    "    blank = [0 for x in range(0,jump+1)]\n",
    "    for i,row in enumerate(history):\n",
    "        if i<jump or i == len(history)-1:\n",
    "            pass\n",
    "        else:\n",
    "            X_pile[i][0:jump] = np.array([history[i-j][Yfin_map['Open']] for j in range(0,jump)])\n",
    "            X_pile[i][jump] = row[Yfin_map['Close']]\n",
    "    return X_pile\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xy(stk, interim, x_meth, y_meth):\n",
    "    #fetch this stock's FULL history in days (where available)\n",
    "    full_set_years = get_X(stk, 'max','1d')\n",
    "    X = x_meth(full_set_years, interim)\n",
    "    y = y_meth(full_set_years)\n",
    "    #drop first #interim entries, because they're blank\n",
    "    \n",
    "    while interim != 0:\n",
    "        X = np.delete(X,interim,0)\n",
    "        y = np.delete(y,interim)\n",
    "        interim = interim -1\n",
    "    \n",
    "    #Drop today because we don't know tomorrow\n",
    "    X = np.delete(X,len(X)-1,0)\n",
    "    y = np.delete(y,len(y)-1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCtraintest(nmc,X,y,modelObj,testFrac):\n",
    "    trainScore = np.zeros(nmc)\n",
    "    testScore  = np.zeros(nmc)\n",
    "    for i in range(nmc):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=testFrac)\n",
    "        modelObj.fit(X_train,y_train)\n",
    "        trainScore[i] = modelObj.score(X_train,y_train)\n",
    "        testScore[i]  = modelObj.score(X_test,y_test)\n",
    "    return trainScore,testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiamondHandsClassifer\t0.8099999999999998\t0.785\n"
     ]
    }
   ],
   "source": [
    "class TO_THE_MOON:\n",
    "    def fit(self,x,y):\n",
    "        return\n",
    "    def score(self,x,y):\n",
    "        ones = np.sum(y==1)\n",
    "        return ones/len(y)\n",
    "\n",
    "#DEMO OF THE PROBLEM\n",
    "A = np.array([1,1,1,1,0])\n",
    "B = np.zeros(A.shape)\n",
    "trains,tests = MCtraintest(500,B,A,TO_THE_MOON(),0.25)\n",
    "print(\"DiamondHandsClassifer\",np.mean(trains),np.mean(tests),sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "8827 8827\n",
      "8827 8827\n",
      "[[26.120001 26.129999 26.139999 26.160000 25.990000 25.709999 26.049999\n",
      "  25.969999 25.750000 26.030001 25.700001 25.469999 26.000000 25.820000\n",
      "  25.870001 26.090000 25.629999 26.030001 26.160000 26.320000 26.350000\n",
      "  26.190001 26.590000 26.510000 27.010000 27.160000 27.450001 27.400000\n",
      "  25.510000 25.360001 26.129999]] [25.889999]\n",
      "[[0.000000 0.019999 -0.100000 -0.060001 -0.010000 0.029999 0.369999\n",
      "  0.240000 0.400000 0.509998 0.299999 0.589998 0.269999 0.029999 0.189999\n",
      "  0.189999 0.199999 0.349998 0.009998 -0.170000 -0.200001 -0.220001\n",
      "  0.000000 -0.350000 -0.430000 -1.090000 -1.110001 -1.410002 0.179998\n",
      "  0.679998 26.129999]] [0.000000]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True,\n",
    "   formatter={'float_kind':'{:f}'.format})\n",
    "\n",
    "\n",
    "X ,y  = make_Xy('MSFT',30, make_X,          make_Y)\n",
    "X2,y2 = make_Xy('MSFT',30, build_increases, find_increases)\n",
    "print(len(X), len(X2))\n",
    "print(len(y), len(y2))\n",
    "\n",
    "r = np.random.choice(len(X),1)\n",
    "print(X[r],y[r])\n",
    "print(X2[r],y2[r])\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIAMONDHANDS\t0.5138681169272603\t0.5118486633439058\n",
      "NaiveBayes\t0.5300475866757307\t0.5338468509288627\n",
      "SVClassifier\t0.5901880806707457\t0.5811961939284096\n",
      "KNeighbors\t0.7208021753908905\t0.5583824195740824\n",
      "DecisionTree\t1.0\t0.6083597643860444\n",
      "RandomForest\t1.0\t0.6867240598096964\n",
      "SGDClass\t0.5909585316111489\t0.5804259175351156\n",
      "TIME: 35.2955022000001\n"
     ]
    }
   ],
   "source": [
    "tick = perf_counter()\n",
    "classifiers = [TO_THE_MOON(),\n",
    "               GaussianNB(), \n",
    "               SVC(),\n",
    "               KNeighborsClassifier(),\n",
    "               DecisionTreeClassifier(),\n",
    "               RandomForestClassifier(n_jobs=3),\n",
    "               SGDClassifier()]\n",
    "\n",
    "Mod_dict = {0: \"DIAMONDHANDS\",1:'NaiveBayes',2:'SVClassifier',3:'KNeighbors',4:'DecisionTree',5:'RandomForest',6:\"SGDClass\"}\n",
    "for i,mod in enumerate(classifiers):\n",
    "    trains,tests = MCtraintest(10,X2,y2,mod,0.5)\n",
    "    print(Mod_dict[i],np.mean(trains),np.mean(tests),sep='\\t')\n",
    "\n",
    "tock=perf_counter()\n",
    "print(\"TIME:\",tock-tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARDRegression\n",
      "AdaBoostRegressor\n",
      "BaggingRegressor\n",
      "BayesianRidge\n",
      "CCA\n",
      "DecisionTreeRegressor\n",
      "DummyRegressor\n",
      "ElasticNet\n",
      "ElasticNetCV\n",
      "ExtraTreeRegressor\n",
      "ExtraTreesRegressor\n",
      "GammaRegressor\n",
      "GaussianProcessRegressor\n",
      "GradientBoostingRegressor\n",
      "HistGradientBoostingRegressor\n",
      "HuberRegressor\n",
      "IsotonicRegression\n",
      "KNeighborsRegressor\n",
      "KernelRidge\n",
      "Lars\n",
      "LarsCV\n",
      "Lasso\n",
      "LassoCV\n",
      "LassoLars\n",
      "LassoLarsCV\n",
      "LassoLarsIC\n",
      "LinearRegression\n",
      "LinearSVR\n",
      "MLPRegressor\n",
      "MultiOutputRegressor\n",
      "MultiTaskElasticNet\n",
      "MultiTaskElasticNetCV\n",
      "MultiTaskLasso\n",
      "MultiTaskLassoCV\n",
      "NuSVR\n",
      "OrthogonalMatchingPursuit\n",
      "OrthogonalMatchingPursuitCV\n",
      "PLSCanonical\n",
      "PLSRegression\n",
      "PassiveAggressiveRegressor\n",
      "PoissonRegressor\n",
      "RANSACRegressor\n",
      "RadiusNeighborsRegressor\n",
      "RandomForestRegressor\n",
      "RegressorChain\n",
      "Ridge\n",
      "RidgeCV\n",
      "SGDRegressor\n",
      "SVR\n",
      "StackingRegressor\n",
      "TheilSenRegressor\n",
      "TransformedTargetRegressor\n",
      "TweedieRegressor\n",
      "VotingRegressor\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import all_estimators\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "estimators = all_estimators(type_filter='regressor')\n",
    "for name, RegressorClass in estimators:\n",
    "    print(name)\n",
    "    #mod = RegressorClass()\n",
    "    #trains,tests = MCtraintest(200,X,y,mod,0.5)\n",
    "    #print(name, np.mean(trains), np.mean(tests),sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994653230315521\t0.9993490407573127\n",
      "0.9987841453242439\t0.9988047837616991\n",
      "0.999476371806644\t0.9991202017873585\n",
      "0.36916190336346727\t0.3546131522438993\n",
      "0.9994602018307008\t0.9993484792927727\n",
      "0.9991561648785652\t0.9991232673696759\n",
      "0.9999045527516056\t0.9993110397786895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "models = [LinearRegression(),\n",
    "          #LogisticRegression(),\n",
    "          Lasso(),\n",
    "          KNeighborsRegressor(),\n",
    "          KernelRidge(),\n",
    "          Ridge(),\n",
    "          SGDRegressor(),\n",
    "          RandomForestRegressor()]\n",
    "\n",
    "for mod in models:\n",
    "    mod = Pipeline([(\"scaler\", StandardScaler()),(\"model\", mod)])\n",
    "    trains,tests = MCtraintest(5,X,y,mod,0.5)\n",
    "    print(np.mean(trains),np.mean(tests),sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_CV_platform(model, param_set,X,y, pNames):\n",
    "    locs = ['rank_test_score','mean_test_score']\n",
    "    locs.extend(pNames)\n",
    "    tick = perf_counter()\n",
    "    print('grid search...')\n",
    "    shuffle = ShuffleSplit(n_splits=25, test_size=.5)\n",
    "    gscv = GridSearchCV(model, param_grid=param_set, n_jobs=-1, refit=True, cv=shuffle, return_train_score=True)\n",
    "    gscv.fit(X,y)\n",
    "    \n",
    "    results = pd.DataFrame(gscv.cv_results_)\n",
    "    tock = perf_counter()\n",
    "    print(results[locs])\n",
    "    print(gscv.best_params_)\n",
    "    print(\"Grid Searched in:\", f\"{tock - tick:0.4f} seconds\")\n",
    "    print()\n",
    "    \n",
    "    print('cross_val...')\n",
    "    CVInfo = cross_validate(gscv.best_estimator_, X, y, cv=10, return_train_score=True,n_jobs=-1)\n",
    "    print('train:', np.mean(CVInfo['train_score']))\n",
    "    print('test:', np.mean(CVInfo['test_score']))\n",
    "    print(np.sum(CVInfo['fit_time']), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search...\n",
      "    rank_test_score  mean_test_score param_max_depth param_max_features  \\\n",
      "0                71         0.671128               5                  3   \n",
      "1                66         0.676330               5                  3   \n",
      "2                64         0.679991               5                  3   \n",
      "3                40         0.688826               5                  5   \n",
      "4                32         0.692143               5                  5   \n",
      "..              ...              ...             ...                ...   \n",
      "85               55         0.682728              25                 20   \n",
      "86               34         0.691119              25                 20   \n",
      "87               77         0.665990              25                 25   \n",
      "88               59         0.682429              25                 25   \n",
      "89               35         0.691110              25                 25   \n",
      "\n",
      "   param_n_estimators  \n",
      "0                  10  \n",
      "1                  25  \n",
      "2                 100  \n",
      "3                  10  \n",
      "4                  25  \n",
      "..                ...  \n",
      "85                 25  \n",
      "86                100  \n",
      "87                 10  \n",
      "88                 25  \n",
      "89                100  \n",
      "\n",
      "[90 rows x 5 columns]\n",
      "{'max_depth': 5, 'max_features': 20, 'n_estimators': 100}\n",
      "Grid Searched in: 603.8036 seconds\n",
      "\n",
      "cross_val...\n",
      "train: 0.72215286742883\n",
      "test: 0.6873037701301736\n",
      "56.036815881729126 seconds\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestClassifier()\n",
    "max_depth = [5,10,15,20,25] \n",
    "max_features = [3, 5,10,15,20,25]\n",
    "n_estimators = [10,25,100]\n",
    "parameters = {'max_depth':max_depth, 'max_features': max_features, 'n_estimators': n_estimators}\n",
    "params = ['param_max_depth','param_max_features','param_n_estimators']\n",
    "\n",
    "general_CV_platform(rfr,parameters,X2,y2, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7217522658610271\n",
      "0.7050294517444495\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(max_depth=5,max_features=20,n_estimators=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2,y2,test_size=0.25)\n",
    "rfc.fit(X_train,y_train)\n",
    "print(rfc.score(X_train,y_train))\n",
    "print(rfc.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7233006042296074 0.7039102854553693\n"
     ]
    }
   ],
   "source": [
    "trains,tests = MCtraintest(100,X2,y2,rfc,0.25)\n",
    "print(np.mean(trains),np.mean(tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
